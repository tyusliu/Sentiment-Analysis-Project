{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff06173",
   "metadata": {},
   "source": [
    "This project was heavily inspired by work by XiaoFan LEI on Medium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b56d9b0",
   "metadata": {},
   "source": [
    "# Split Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "343e5071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train size 1120 y train size 1120\n",
      "test size 480\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create input and output directories\n",
    "import os\n",
    "inputpath = 'input'\n",
    "outputpath = 'output'\n",
    "if os.path.exists(inputpath) is False:\n",
    "    os.mkdir(inputpath)\n",
    "if os.path.exists(outputpath) is False:\n",
    "    os.mkdir(outputpath)\n",
    "\n",
    "#input file path\n",
    "sentiment140_file = 'input/training.1600000.processed.noemoticon.csv'\n",
    "\n",
    "# read csv\n",
    "colnames = ['polarity', 'id', 'date', 'query', 'user', 'tweet']\n",
    "df_tweets = pd.read_csv(sentiment140_file, encoding='UTF', names=colnames, encoding_errors='ignore')\n",
    "\n",
    "# get 1600 tweets\n",
    "df = df_tweets[['polarity','tweet']].sample(n=1600, random_state=0)\n",
    "df.to_csv(\"output/selected_tweets.csv\", index=False)\n",
    "\n",
    "# ---------\n",
    "\n",
    "# X is the list of tweets\n",
    "x = df.tweet.values\n",
    "# Y is their polarity\n",
    "y = df.polarity.replace(4, 1) # Positive is 1, 0 is negative\n",
    "\n",
    "# split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print('x train size', len(x_train), 'y train size', len(y_train))\n",
    "print('test size', len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ced65a",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b493d7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from autocorrect import Speller\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "spell = Speller(lang='en')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "# Fixing Word Lengthening: hiiiiii -> hii; helllllooo -> hellloo\n",
    "def reduce_length(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "def text_preprocess(doc):\n",
    "    #Lowercasing all the letters\n",
    "    temp = doc.lower()\n",
    "    #Removing hashtags and mentions\n",
    "    temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp)\n",
    "    temp = re.sub(\"#[A-Za-z0-9_]+\",\"\", temp)\n",
    "    #Removing links\n",
    "    temp = re.sub(r\"http\\S+\", \"\", temp) # \\S stops at whitespace\n",
    "    temp = re.sub(r\"www.\\S+\", \"\", temp) # \\S stops at whitespace\n",
    "    #removing numbers\n",
    "    temp = re.sub(\"[0-9]\",\"\", temp)\n",
    "    #Removing '\n",
    "    temp = re.sub(\"'\",\" \",temp)\n",
    "\n",
    "    #Tokenization\n",
    "    temp = word_tokenize(temp)\n",
    "    #Fixing Word Lengthening\n",
    "    temp = [reduce_length(w) for w in temp]\n",
    "    #spell corrector\n",
    "    temp = [spell(w) for w in temp]\n",
    "    #stem\n",
    "    temp = [lemm.lemmatize(w) for w in temp]\n",
    "    #Removing short words\n",
    "    temp = [w for w in temp if len(w)>2]\n",
    "    temp = \" \".join(w for w in temp) # back to a string\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484ce5f",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd5439b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score on test set: 0.6895833333333333\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.classify import NaiveBayesClassifier, accuracy\n",
    "\n",
    "# build the dataset (list of tuples)\n",
    "# dataset = [(['this', 'is', 'a', 'tweet'], 0), (...), ... ]\n",
    "def build_dataset(x, y):\n",
    "    words = [text_preprocess(word).split(\" \") for word in x]\n",
    "    dataset = list(zip(words, y)) # list of tuples\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "all_words = FreqDist(sum([w.split(\" \") for w in x_train],[]))\n",
    "word_features = list(all_words)[:2000] # list of the 2000 most common words\n",
    "\n",
    "# Says whether or not the most commonly used words are within the given list of words\n",
    "# So the dimension of the feature vector is as long as 2000 most common words\n",
    "# All features are false if none of the most common words are in that tweet\n",
    "def document_features(words):\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in set(words))\n",
    "\n",
    "    return features\n",
    "\n",
    "trainset = build_dataset(x_train, y_train)\n",
    "testset = build_dataset(x_test, y_test)\n",
    "train_set = [(document_features(d), y) for (d,y) in trainset]\n",
    "test_set = [(document_features(d), y) for (d,y) in testset]\n",
    "\n",
    "\n",
    "nb_classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Test the classifier\n",
    "print(\"accuracy score on test set:\", accuracy(nb_classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a788cc",
   "metadata": {},
   "source": [
    "## Naive Bayes Accuracy: ~69%\n",
    "Decent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aa11cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                    look = True                1 : 0      =      6.5 : 1.0\n",
      "                    miss = True                0 : 1      =      6.2 : 1.0\n",
      "                   those = True                1 : 0      =      5.9 : 1.0\n",
      "                  thanks = True                1 : 0      =      5.4 : 1.0\n",
      "                    dont = True                0 : 1      =      5.2 : 1.0\n",
      "                    wish = True                0 : 1      =      5.2 : 1.0\n",
      "                  follow = True                1 : 0      =      5.2 : 1.0\n",
      "                   later = True                1 : 0      =      5.2 : 1.0\n",
      "                   lunch = True                1 : 0      =      5.2 : 1.0\n",
      "                     yes = True                1 : 0      =      5.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "nb_classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d774a131",
   "metadata": {},
   "source": [
    "Let's take a look at trying our classifier on a brand new sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46df10be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature length is: 2000\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"This is the best band I've ever heard!\"\n",
    "test_sent_features = {word: (word in text_preprocess(test_sentence).split(\" \")) for word in word_features}\n",
    "print('feature length is:', len(test_sent_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cad1c78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classifier.classify(test_sent_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25553dd0",
   "metadata": {},
   "source": [
    "Good! It looks like our classifier correctly identifies the sentence as positive. Now, let's see if we can do better..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3101d109",
   "metadata": {},
   "source": [
    "# RNN Classifier: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f76b1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size: 2653\n",
      "text_to_sequences: [[685, 45, 5, 6, 686, 76, 337, 31, 165, 1069, 1070], [687, 295, 19, 12, 1071, 41, 77, 256, 179, 504, 17]]\n",
      "longest tweet contains 27 words\n",
      "pad_sequences will produce an array that is (1120 x 37)\n",
      "Xtrain is 1120 x 37\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, Bidirectional, LSTM\n",
    "\n",
    "#################data preparation\n",
    "cleaned_train = [text_preprocess(d) for d in x_train]\n",
    "#building a dictionary\n",
    "tk = Tokenizer(num_words=None) #the maximum number of words to keep, based on word frequency\n",
    "tk.fit_on_texts(cleaned_train)\n",
    "#1.1 get the size of the dictionary\n",
    "dico_size = len(tk.word_counts.items()) # This gets the number of unique words in our training set\n",
    "print('dictionary size:', dico_size)\n",
    "num_tokens = dico_size + 1\n",
    "#2. building sequneces\n",
    "seq_X = tk.texts_to_sequences(cleaned_train) # This just converts each word to a single number that corresponds to a dictionary index in the Tokenizer\n",
    "print('text_to_sequences:', seq_X[:2])\n",
    "#2.1 calculate maxi length of tweets\n",
    "max_len = np.max(np.array([len(d) for d in seq_X]))\n",
    "marg_len=10\n",
    "print(f'longest tweet contains {max_len} words')\n",
    "maxlen = max_len + marg_len\n",
    "print(f'pad_sequences will produce an array that is ({len(seq_X)} x {maxlen})')\n",
    "#3. padding the sequences\n",
    "Xtrain = pad_sequences(seq_X,maxlen=maxlen,padding='post') # padding='post' means padding goes after the tweet is over\n",
    "print(f'Xtrain is {len(Xtrain)} x {len(Xtrain[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c437a09",
   "metadata": {},
   "source": [
    "Now we make a function that maps each word to a vector (np array) of it's embedding (can think of it as coordinates of how similar the words are).\n",
    "\n",
    "This one is pre-trained from wiki-news. So we just load it. It returns a dictionary.\n",
    "\n",
    "\n",
    "EX: embedding_model['my_word'] -> [123, 13, 3 , 23, 1, ... , 123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0508ca5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 999995 word vectors in loaded fasttext model.\n"
     ]
    }
   ],
   "source": [
    "###########loading glove and fasttext embedding vectors\n",
    "def load_embedding_model(file):\n",
    "    embedding_model = {}\n",
    "    with open(file,'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "            embedding_model[word] = embedding\n",
    "    return embedding_model\n",
    "\n",
    "embedding_index_fasttext = load_embedding_model('input/wiki-news-300d-1M.vec')\n",
    "print('found %s word vectors in loaded fasttext model.' % len(embedding_index_fasttext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e3f7431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 2579 words (74 misses)\n",
      "words not included in pretrained model: ['loveyouu', 'lmaz', 'gottabesomebody', 'auctionsniper', 'funcionou', 'ncis', 'spartak', 'twitterberry', 'copiedandpasted', 'omgosh', 'catal', 'cookiedough', 'tagaytay', 'arrgghh', 'krystinas', 'kutnerr', 'camila', 'obnoxciously', 'rushmore', 'honeytint', 'cahntilli', 'fuckingtastic', 'allyssas', 'huhuhu', '…needed', 'bassotti', 'citrixcloud', 'wolfmother', 'kirstie', 'cheol', 'vilmarie', 'xaviermedia', 'heartburny', 'pnas', 'doliviawilder', 'techhelp', 'grimshaw', 'gyokoro', 'reblipping', 'followfriday', 'mbb', 'organization�', 'twitpics', 'lastlatter', 'pawngame', 'arangurens', 'konstantino', 'goodmorning', 'beeteedubs', 'pahonorsocietyst', 'farewellness', 'bellarlly', 'saymyspacetwitters', 'gottwitter', 'wwd', 'fianc�', 'triginometery', 'atikah', 'ilovejb', 'lismore', 'bodypump', 'groundctrl', 'lemmiin', 'tysonritteraar', 'anacecii', 'btnreply', 'wesseltof', 'maryanne', 'maltesers', 'elyshia', 'muhhwahh', 'haahhaahaa', 'archuleta', 'makeupblog']\n"
     ]
    }
   ],
   "source": [
    "##########Preparing a corresponding embedding matrix\n",
    "def embedding_matrix(num_tokens,embedding_dim,embedding_index):\n",
    "    hits=0\n",
    "    misses=[]\n",
    "\n",
    "    # Prepare embedding matrix\n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim)) # Each word in train set will be mapped to a vector of dimension 300\n",
    "\n",
    "    for word, i in tk.word_index.items(): # Going through all the words and their indecies in the training set\n",
    "        embedding_vector = embedding_index.get(word) # Get the word vector of that SAME WORD in the wiki embeddings\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            # This includes the representation for \"padding\" and \"OOV\"\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses.append(word) # If the word in the tweet is not in the wiki dictionary add it to a list and it will not be included in the model\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, len(misses)))\n",
    "    print(\"words not included in pretrained model:\",misses)\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_dim=300\n",
    "embedding_matrix_fasttext = embedding_matrix(num_tokens,embedding_dim,embedding_index_fasttext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9a7a3d",
   "metadata": {},
   "source": [
    "## Building the LSTM NN\n",
    "- Embedding layer\n",
    "- Dropout\n",
    "- Bidirectional LSTM\n",
    "- 64 Dense ReLU\n",
    "- 16 Dense ReLU\n",
    "- Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c13d997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 37, 300)           796200    \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (None, 37, 300)           0         \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 16)                19776     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                1088      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                1040      \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 818121 (3.12 MB)\n",
      "Trainable params: 21921 (85.63 KB)\n",
      "Non-trainable params: 796200 (3.04 MB)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "##########Building the model with the embedding layer non trainable\n",
    "embedding_layer_fasttext = Embedding( # Turns positive integers (indexes) into dense vectors of fixed size.\n",
    "    input_dim=num_tokens, # Num words in train set\n",
    "    output_dim=embedding_dim, # Each word represented by a 300 dim vector\n",
    "    input_length=maxlen, # the length of input sequences. it takes each word for each input at the same time as one sequence\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix_fasttext), # embedding layer is set to pre-trained embeddings (?)\n",
    "    trainable=False, # word embeddings are pre-trained so no need to adjust them\n",
    ")\n",
    "lstm_model = Sequential() # Makes a new sequential model with one input tensor and one output tensor(LSTM)\n",
    "# add embedding lyaer\n",
    "lstm_model.add(embedding_layer_fasttext)\n",
    "lstm_model.add(Dropout(0.5))\n",
    "#LSTM\n",
    "lstm_model.add(Bidirectional(LSTM(8,dropout=0.5,recurrent_dropout=0.2)))\n",
    "# add a vanilla hidden layer:\n",
    "lstm_model.add(Dense(64, activation='relu'))\n",
    "lstm_model.add(Dense(16, activation='relu'))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(units=1, activation='sigmoid',name='predictions'))\n",
    "\n",
    "#compiling the model\n",
    "lstm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-3, epsilon=1e-08, clipnorm=1.0), \n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(lstm_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b045be5",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "985b92a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "WARNING:tensorflow:From C:\\Users\\tyusl\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\tyusl\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "28/28 - 5s - loss: 0.6915 - accuracy: 0.5078 - val_loss: 0.6915 - val_accuracy: 0.5000 - 5s/epoch - 166ms/step\n",
      "Epoch 2/15\n",
      "28/28 - 1s - loss: 0.6887 - accuracy: 0.5391 - val_loss: 0.6717 - val_accuracy: 0.6161 - 677ms/epoch - 24ms/step\n",
      "Epoch 3/15\n",
      "28/28 - 1s - loss: 0.6767 - accuracy: 0.5625 - val_loss: 0.6534 - val_accuracy: 0.6161 - 663ms/epoch - 24ms/step\n",
      "Epoch 4/15\n",
      "28/28 - 1s - loss: 0.6279 - accuracy: 0.6719 - val_loss: 0.6035 - val_accuracy: 0.6562 - 616ms/epoch - 22ms/step\n",
      "Epoch 5/15\n",
      "28/28 - 1s - loss: 0.6108 - accuracy: 0.6853 - val_loss: 0.6480 - val_accuracy: 0.6518 - 631ms/epoch - 23ms/step\n",
      "Epoch 6/15\n",
      "28/28 - 1s - loss: 0.6345 - accuracy: 0.6395 - val_loss: 0.6076 - val_accuracy: 0.6473 - 668ms/epoch - 24ms/step\n",
      "Epoch 7/15\n",
      "28/28 - 1s - loss: 0.6139 - accuracy: 0.6663 - val_loss: 0.5928 - val_accuracy: 0.6920 - 665ms/epoch - 24ms/step\n",
      "Epoch 8/15\n",
      "28/28 - 1s - loss: 0.6081 - accuracy: 0.6685 - val_loss: 0.5913 - val_accuracy: 0.6964 - 641ms/epoch - 23ms/step\n",
      "Epoch 9/15\n",
      "28/28 - 1s - loss: 0.5948 - accuracy: 0.6786 - val_loss: 0.5858 - val_accuracy: 0.6830 - 652ms/epoch - 23ms/step\n",
      "Epoch 10/15\n",
      "28/28 - 1s - loss: 0.5944 - accuracy: 0.6853 - val_loss: 0.5791 - val_accuracy: 0.7009 - 642ms/epoch - 23ms/step\n",
      "Epoch 11/15\n",
      "28/28 - 1s - loss: 0.5585 - accuracy: 0.7266 - val_loss: 0.5939 - val_accuracy: 0.6830 - 610ms/epoch - 22ms/step\n",
      "Epoch 12/15\n",
      "28/28 - 1s - loss: 0.5688 - accuracy: 0.7009 - val_loss: 0.5483 - val_accuracy: 0.7009 - 602ms/epoch - 22ms/step\n",
      "Epoch 13/15\n",
      "28/28 - 1s - loss: 0.5713 - accuracy: 0.6886 - val_loss: 0.5544 - val_accuracy: 0.6830 - 604ms/epoch - 22ms/step\n",
      "Epoch 14/15\n",
      "28/28 - 1s - loss: 0.5772 - accuracy: 0.6864 - val_loss: 0.5991 - val_accuracy: 0.6830 - 599ms/epoch - 21ms/step\n",
      "Epoch 15/15\n",
      "28/28 - 1s - loss: 0.5881 - accuracy: 0.6808 - val_loss: 0.5550 - val_accuracy: 0.7009 - 618ms/epoch - 22ms/step\n",
      "LSTM model evaluation with fasttext 300d embedding:\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.5746 - accuracy: 0.7063\n",
      "[0.574580729007721, 0.706250011920929]\n"
     ]
    }
   ],
   "source": [
    "###########Trainning and evoluating the model\n",
    "parameterization = {\n",
    "    'batch_size': 32,\n",
    "    # Add other parameters as needed\n",
    "}\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "history = lstm_model.fit(Xtrain, y_train, \n",
    "                            batch_size=parameterization.get('batch_size'), \n",
    "                            epochs=NUM_EPOCHS,\n",
    "                             validation_split=0.2, verbose=2)\n",
    "#test\n",
    "cleaned_test = [text_preprocess(d) for d in x_test]\n",
    "Qtest = tk.texts_to_sequences(cleaned_test)\n",
    "Ptest = pad_sequences(Qtest,maxlen=maxlen,padding='post' )\n",
    "print(\"LSTM model evaluation with fasttext 300d embedding:\")\n",
    "print(lstm_model.evaluate(Ptest, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c94774e",
   "metadata": {},
   "source": [
    "After 15 epochs, it looks like we are achieving about the same accuracy as out naive bayes classifier. Next, we'll use BERT to perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d3beb2",
   "metadata": {},
   "source": [
    "# Using BERT for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "def482f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "# Hide GPU from visible devices\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "#load the model\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68a2ed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################text cleaning\n",
    "def preprocess(X):\n",
    "    import re\n",
    "    def text_clean(text):\n",
    "        temp = text.lower()\n",
    "        temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp)\n",
    "        temp = re.sub(\"#[A-Za-z0-9_]+\",\"\", temp)\n",
    "        temp = re.sub(r\"http\\S+\", \"\", temp)\n",
    "        temp = re.sub(r\"www.\\S+\", \"\", temp)\n",
    "        temp = re.sub(\"[0-9]\",\"\", temp)\n",
    "        return temp\n",
    "    X_cleaned = [text_clean(text) for text in X]\n",
    "    return X_cleaned\n",
    "\n",
    "############transforming raw data to an appropriate format ready to feed into the BERT model\n",
    "def convert_example_to_feature(text):\n",
    "    return bert_tokenizer.encode_plus(text,\n",
    "            add_special_tokens = True, # add [CLS], [SEP]\n",
    "            max_length = 128, # max length of the text that can go to BERT\n",
    "            pad_to_max_length = True, # add [PAD] tokens\n",
    "            return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "          )\n",
    "\n",
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "    return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "    }, label\n",
    "\n",
    "def encode_examples(X,y):\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "    for text, label in zip(X, y):\n",
    "        bert_input = convert_example_to_feature(text)\n",
    "        input_ids_list.append(bert_input['input_ids'])\n",
    "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "        attention_mask_list.append(bert_input['attention_mask'])\n",
    "        label_list.append([label])\n",
    "    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab37235e",
   "metadata": {},
   "source": [
    "## Encoding Train, Validation, and Test for BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13ddd02",
   "metadata": {},
   "source": [
    "Not really sure why, but the length of y_train changed from the beginning of file, so re run dataset code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "945d909c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train size 2100 y train size 2100\n",
      "test size 900\n"
     ]
    }
   ],
   "source": [
    "#input file path\n",
    "sentiment140_file = 'input/training.1600000.processed.noemoticon.csv'\n",
    "\n",
    "# read csv\n",
    "colnames = ['polarity', 'id', 'date', 'query', 'user', 'tweet']\n",
    "df_tweets = pd.read_csv(sentiment140_file, encoding='UTF', names=colnames, encoding_errors='ignore')\n",
    "\n",
    "# get 1600 tweets\n",
    "df = df_tweets[['polarity','tweet']].sample(n=3000, random_state=0)\n",
    "df.to_csv(\"output/selected_tweets.csv\", index=False)\n",
    "\n",
    "# ---------\n",
    "\n",
    "# X is the list of tweets\n",
    "x = df.tweet.values\n",
    "# Y is their polarity\n",
    "y = df.polarity.replace(4, 1) # Positive is 1, 0 is negative\n",
    "\n",
    "# split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print('x train size', len(x_train), 'y train size', len(y_train))\n",
    "print('test size', len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c9bc965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyusl\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# train dataset\n",
    "# Splitting train set into train and validation to help with training\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=0.2, random_state=0)\n",
    "ds_train_encoded = encode_examples(preprocess(X_train), y_train).shuffle(100).batch(32).repeat(2)\n",
    "ds_val_encoded = encode_examples(preprocess(X_validation), y_validation).batch(32)\n",
    "# test dataset\n",
    "ds_test_encoded = encode_examples(preprocess(x_test), y_test).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6565ccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### compiling the model\n",
    "learning_rate = 3e-5\n",
    "# choosing Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "bert_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f678822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "106/106 [==============================] - 1425s 13s/step - loss: 0.3273 - accuracy: 0.8723 - val_loss: 0.3489 - val_accuracy: 0.8643\n",
      "Epoch 2/4\n",
      "106/106 [==============================] - 1109s 10s/step - loss: 0.1007 - accuracy: 0.9688 - val_loss: 0.5332 - val_accuracy: 0.8405\n",
      "Epoch 3/4\n",
      "106/106 [==============================] - 1025s 10s/step - loss: 0.0334 - accuracy: 0.9893 - val_loss: 0.7186 - val_accuracy: 0.8190\n",
      "Epoch 4/4\n",
      "106/106 [==============================] - 1024s 10s/step - loss: 0.0245 - accuracy: 0.9929 - val_loss: 0.6635 - val_accuracy: 0.8524\n",
      "accuracy: 83.78%\n"
     ]
    }
   ],
   "source": [
    "#############training and evaluating\n",
    "bert_model.fit(ds_train_encoded, epochs=4, validation_data=ds_val_encoded)\n",
    "\n",
    "loss, acc = bert_model.evaluate(ds_test_encoded, verbose=0)\n",
    "print(\"accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "925e1384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: outputs/bert_model\\saved_model\\1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: outputs/bert_model\\saved_model\\1\\assets\n"
     ]
    }
   ],
   "source": [
    "##################Saving the model\n",
    "bert_model.save_pretrained(\"outputs/bert_model\", saved_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0018088",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
